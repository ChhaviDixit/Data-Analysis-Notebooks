{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import Birch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Date_Time  \\\n",
      "0      Thu Apr 09 01:31:50 +0000 2015   \n",
      "1      Wed Apr 08 23:30:18 +0000 2015   \n",
      "2      Wed Apr 08 23:30:18 +0000 2015   \n",
      "3      Wed Apr 08 18:05:28 +0000 2015   \n",
      "4      Wed Apr 08 13:19:33 +0000 2015   \n",
      "...                               ...   \n",
      "38395  Wed Nov 26 16:55:34 +0000 2014   \n",
      "38396  Wed Nov 26 16:35:06 +0000 2014   \n",
      "38397  Wed Nov 26 16:15:07 +0000 2014   \n",
      "38398  Wed Nov 26 15:50:18 +0000 2014   \n",
      "38399  Wed Nov 26 15:33:51 +0000 2014   \n",
      "\n",
      "                                                    News  \n",
      "0                       Breast cancer risk test devised   \n",
      "1                   GP workload harming care - BMA poll   \n",
      "2                   Short people's 'heart risk greater'   \n",
      "3                  New approach against HIV 'promising'   \n",
      "4                  Coalition 'undermined NHS' - doctors   \n",
      "...                                                  ...  \n",
      "38395  Researchers use video games to study how sleep...  \n",
      "38396        Are energy drinks really that bad for you?   \n",
      "38397  Men suffering from #depression may also suffer...  \n",
      "38398  #Thanksgiving science: Why #gratitude is good ...  \n",
      "38399  Clinton KellyÆs fresh and #fruity take on #hol...  \n",
      "\n",
      "[38400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#due to memory constraints, only 2500 or lesser rows have been considered from each text file\n",
    "\n",
    "#creating array of all text file names with encoding \"utf8\" and appending details to be added in dataframe to relevant lists\n",
    "arr=['bbchealth.txt', 'cbchealth.txt', 'goodhealth.txt', 'cnnhealth.txt', 'everydayhealth.txt', 'usnewshealth.txt', 'reuters_health.txt', 'nytimeshealth.txt', 'nprhealth.txt', 'latimeshealth.txt']\n",
    "time=[]\n",
    "text=[]\n",
    "for i in arr:\n",
    "    s='tweet/'+i\n",
    "    with open(s,encoding=\"utf8\") as file:\n",
    "        contents = file.readlines()\n",
    "    for i in range(min(len(contents),2500)):\n",
    "        a=contents[i].split('|')\n",
    "        time.append(a[1])\n",
    "        if 'http' in a[2]:\n",
    "            l=a[2].split('http')\n",
    "            text.append(l[0])\n",
    "        else:\n",
    "            text.append(l)\n",
    "\n",
    "#creating array of text files with encoding \"IBM437\" and appending all setails to be added in dataframe to lists\n",
    "arr=['gdnhealthcare.txt', 'KaiserHealthNews.txt', 'msnhealthnews.txt', 'NBChealth.txt', 'wsjhealth.txt', 'foxnewshealth.txt']\n",
    "for i in arr:\n",
    "    s='tweet/'+i\n",
    "    with open(s,encoding=\"IBM437\") as file:\n",
    "        contents = file.readlines()\n",
    "    for i in range(min(len(contents),2500)):\n",
    "        a=contents[i].split('|')\n",
    "        time.append(a[1])\n",
    "        if 'http' in a[2]:\n",
    "            l=a[2].split('http')\n",
    "            text.append(l[0])\n",
    "        else:\n",
    "            text.append(l)\n",
    "\n",
    "#dataframe with name df is created containg time stamp and news content\n",
    "df = pd.DataFrame(list(zip(time, text)), columns =['Date_Time', 'News']) \n",
    "print(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Date_Time  \\\n",
      "0      Thu Apr 09 01:31:50 +0000 2015   \n",
      "1      Wed Apr 08 23:30:18 +0000 2015   \n",
      "2      Wed Apr 08 23:30:18 +0000 2015   \n",
      "3      Wed Apr 08 18:05:28 +0000 2015   \n",
      "4      Wed Apr 08 13:19:33 +0000 2015   \n",
      "...                               ...   \n",
      "38395  Wed Nov 26 16:55:34 +0000 2014   \n",
      "38396  Wed Nov 26 16:35:06 +0000 2014   \n",
      "38397  Wed Nov 26 16:15:07 +0000 2014   \n",
      "38398  Wed Nov 26 15:50:18 +0000 2014   \n",
      "38399  Wed Nov 26 15:33:51 +0000 2014   \n",
      "\n",
      "                                                    News  \\\n",
      "0                       Breast cancer risk test devised    \n",
      "1                   GP workload harming care - BMA poll    \n",
      "2                   Short people's 'heart risk greater'    \n",
      "3                  New approach against HIV 'promising'    \n",
      "4                  Coalition 'undermined NHS' - doctors    \n",
      "...                                                  ...   \n",
      "38395  Researchers use video games to study how sleep...   \n",
      "38396        Are energy drinks really that bad for you?    \n",
      "38397  Men suffering from #depression may also suffer...   \n",
      "38398  #Thanksgiving science: Why #gratitude is good ...   \n",
      "38399  Clinton KellyÆs fresh and #fruity take on #hol...   \n",
      "\n",
      "                                  Lemmatized_News_Tokens  \n",
      "0                  [breast, cancer, risk, test, devised]  \n",
      "1               [gp, workload, harming, care, bma, poll]  \n",
      "2                  [short, people, heart, risk, greater]  \n",
      "3               [new, approach, against, hiv, promising]  \n",
      "4                    [coalition, undermined, nh, doctor]  \n",
      "...                                                  ...  \n",
      "38395  [researcher, use, video, game, to, study, how,...  \n",
      "38396  [are, energy, drink, really, that, bad, for, you]  \n",
      "38397  [men, suffering, from, depression, may, also, ...  \n",
      "38398  [thanksgiving, science, why, gratitude, is, go...  \n",
      "38399  [clinton, kellyæs, fresh, and, fruity, take, o...  \n",
      "\n",
      "[38400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#function to remove punctuation marks, convert all letters to lower case, remove extra spaces and tokenise each term\n",
    "def normalize(l):\n",
    "    line=''\n",
    "    for i in l:\n",
    "        line+=i\n",
    "    line = line.lower().strip()\n",
    "    line = ''.join([char for char in line if char not in string.punctuation+string.digits])\n",
    "    return word_tokenize(line)\n",
    "\n",
    "#function to lemmatise each token\n",
    "def lemmatize_sent(line_tokens):\n",
    "    return list(map(lemmatizer.lemmatize, line_tokens))\n",
    "\n",
    "#adding column News_Tokens with all tokens, \n",
    "#using that column to add new column with lemmatised tokens and dropping the News_Tokens column\n",
    "df['News_Tokens'] = df['News'].map(normalize)\n",
    "df['Lemmatized_News_Tokens'] = df['News_Tokens'].map(lemmatize_sent)\n",
    "df=df.drop(['News_Tokens'], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5093)\t0.7313449658145135\n",
      "  (0, 19280)\t0.35774613697037033\n",
      "  (0, 16523)\t0.309846095250083\n",
      "  (0, 2794)\t0.2909615380609007\n",
      "  (0, 2379)\t0.39558693493094116\n",
      "  (1, 14874)\t0.36681022361290583\n",
      "  (1, 2161)\t0.50640777410857\n",
      "  (1, 2879)\t0.23155800813840297\n",
      "  (1, 8413)\t0.4607796976818966\n",
      "  (1, 21366)\t0.46602628317442485\n",
      "  (1, 7950)\t0.35480119593546994\n",
      "  (2, 8019)\t0.5568122696967475\n",
      "  (2, 8566)\t0.3604703368646069\n",
      "  (2, 14440)\t0.3642907725319106\n",
      "  (2, 17463)\t0.5596358627101303\n",
      "  (2, 16523)\t0.33781824517433257\n",
      "  (3, 15319)\t0.5463074760536667\n",
      "  (3, 8812)\t0.4312945802962516\n",
      "  (3, 368)\t0.4229229380745659\n",
      "  (3, 969)\t0.5112411992404181\n",
      "  (3, 12869)\t0.2744116456430897\n",
      "  (4, 5419)\t0.2951793976827633\n",
      "  (4, 12917)\t0.29345480457718437\n",
      "  (4, 20217)\t0.673764538359828\n",
      "  (4, 3665)\t0.6105691997660672\n",
      "  :\t:\n",
      "  (38397, 1606)\t0.4383219405635086\n",
      "  (38397, 4980)\t0.3238590543881433\n",
      "  (38397, 11911)\t0.30156850676299746\n",
      "  (38397, 19332)\t0.1312660612594197\n",
      "  (38397, 9292)\t0.14000045695859767\n",
      "  (38397, 7433)\t0.20336522736150767\n",
      "  (38397, 11739)\t0.20253890266887084\n",
      "  (38398, 8009)\t0.5899326110755888\n",
      "  (38398, 19325)\t0.43581483800441656\n",
      "  (38398, 21763)\t0.20928792793478904\n",
      "  (38398, 17027)\t0.38494437797532755\n",
      "  (38398, 7879)\t0.2991703661800326\n",
      "  (38398, 8504)\t0.2039082786111781\n",
      "  (38398, 21191)\t0.2639998127320674\n",
      "  (38398, 7258)\t0.16416794960833545\n",
      "  (38398, 9770)\t0.205749409629852\n",
      "  (38399, 10262)\t0.4686144230090063\n",
      "  (38399, 7456)\t0.4511227363539349\n",
      "  (38399, 3502)\t0.39131826467068276\n",
      "  (38399, 5303)\t0.3579630018394311\n",
      "  (38399, 7408)\t0.34513962837949674\n",
      "  (38399, 18991)\t0.23024744763697638\n",
      "  (38399, 13502)\t0.14581297040469965\n",
      "  (38399, 8857)\t0.28533515669624465\n",
      "  (38399, 723)\t0.14425144760213557\n"
     ]
    }
   ],
   "source": [
    "#creating TF-IDF matrix for the lemmatised tokens in news content\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(df['Lemmatized_News_Tokens'].map(lambda x: ' '.join(x)).tolist())\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1f2269acb4a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#reducing the basis of the dataframe by Principal Component Analysis to 2 dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpca_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#defining kmeans class with number of clusters 4 and clutering the resultant matrix from PCA Analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \"\"\"\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'arpack'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'randomized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    533\u001b[0m                                      \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterated_power\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m                                      \u001b[0mflip_sign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m                                      random_state=random_state)\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[1;32m--> 348\u001b[1;33m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[1;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LU'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'QR'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'economic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#reducing the basis of the dataframe by Principal Component Analysis to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca_matrix = pca.fit_transform(tfidf_matrix.A)\n",
    "\n",
    "#defining kmeans class with number of clusters 4 and clutering the resultant matrix from PCA Analysis\n",
    "kmeans=KMeans(n_clusters=4)\n",
    "kmeans.fit(pca_matrix)\n",
    "\n",
    "#plotting the clusters as a Scatter Plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "sb.scatterplot(x=pca_matrix[:,0], y=pca_matrix[:,1], hue=kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating brc class for BIRCH Clustering, defining threshold as 0.05 and number of clusters as 4\n",
    "brc = Birch(threshold=0.05, n_clusters=4)\n",
    "\n",
    "#Clustering using the resultant of PCA Analysis\n",
    "brc.fit(pca_matrix)\n",
    "\n",
    "#Creating the plot of clusters formed from Birch Algorithm\n",
    "plt.figure(figsize=(10, 10))\n",
    "sb.scatterplot(x=pca_matrix[:,0], y=pca_matrix[:,1], cmap = 'rainbow', hue=brc.labels_) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
