{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Terms                                                Doc\n",
      "0             http                                      [0, 690, 852]\n",
      "1             book                                                 []\n",
      "2         toscrape        [2, 510, 677, 680, 684, 692, 854, 858, 863]\n",
      "3         comindex                                                [3]\n",
      "4    htmlcatalogue  [4, 9, 15, 21, 28, 35, 41, 47, 53, 60, 66, 72,...\n",
      "..             ...                                                ...\n",
      "270      loginhttp                                              [856]\n",
      "271         search                                              [860]\n",
      "272       aspxhttp                                              [861]\n",
      "273         random                                              [865]\n",
      "274        mistake                                                 []\n",
      "\n",
      "[275 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "f = open('Links.txt','w')\n",
    "\n",
    "MAX_THREADS = 5\n",
    "\n",
    "def download_url(url):\n",
    "    print(url)\n",
    "    resp = requests.get(url)\n",
    "    title = ''.join(x for x in url if x.isalpha()) + \"html\"\n",
    "    \n",
    "    with open(title, \"wb\") as fh:\n",
    "        fh.write(resp.content)\n",
    "        \n",
    "    time.sleep(0.25)\n",
    "\n",
    "def get_links_recursive(base, path, visited, max_depth=1, depth=0):\n",
    "    if depth <= max_depth:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n",
    "\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                href = link.get(\"href\")\n",
    "\n",
    "                if href not in visited:\n",
    "                    visited.add(href)\n",
    "                    f.write(href)\n",
    "                    #print(f\"at depth {depth}: {href}\")\n",
    "\n",
    "                    if href.startswith(\"http\"):\n",
    "                        get_links_recursive(href, \"\", visited, max_depth, depth + 1)\n",
    "                    else:\n",
    "                        get_links_recursive(base, href, visited, max_depth, depth + 1)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "def download_stories(story_urls):\n",
    "    threads = min(MAX_THREADS, len(story_urls))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        for url in story_urls:\n",
    "            get_links_recursive(url, \"\", set([url]))\n",
    "\n",
    "def main(story_urls):\n",
    "    download_stories(story_urls)\n",
    "\n",
    "urls = [\"http://toscrape.com/\"]\n",
    "main(urls)\n",
    "\n",
    "f.close()\n",
    "\n",
    "file = open('Links.txt', encoding='utf8') \n",
    "read = file.read() \n",
    "file.seek(0) \n",
    "read \n",
    "\n",
    "\n",
    "array = [] \n",
    "for i in range(line): \n",
    "    array.append(file.readline()) \n",
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "for ele in read:   \n",
    "    if ele in punc:   \n",
    "        read = read.replace(ele, \" \")   \n",
    "         \n",
    "read=read.lower() \n",
    " \n",
    "for i in range(1): \n",
    "    text_tokens = word_tokenize(read) \n",
    "\n",
    "stop_words=list(set(stopwords.words('english')))\n",
    "stop_words_add=[ ',','.', '-', '(', ')', '[', ']', ':', ';', '\\'','&','_']\n",
    "for words in stop_words_add:\n",
    "    stop_words.extend(words)\n",
    "\n",
    "final_term_stop=[]\n",
    "for w in text_tokens:\n",
    "    if w not in stop_words:\n",
    "        w=w.lower()\n",
    "    final_term_stop.append(w)\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "final_term=[]\n",
    "for w in final_term_stop:\n",
    "    final_term.append(lemmatizer.lemmatize(w))\n",
    "common=[]\n",
    "for w in final_term:\n",
    "    if w not in common:\n",
    "        common.append(w)\n",
    "f1=[]\n",
    "for w in common:\n",
    "    l1=[]\n",
    "    for i in range(0, len(text_tokens)):\n",
    "        if text_tokens[i] == w:\n",
    "            l1.append(i)\n",
    "    f1.append(l1)\n",
    "a=pd.DataFrame(zip(common,f1))\n",
    "a.columns=['Terms', 'Doc']\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
